import torch
import torch.nn as nn
import dgl
import torch.nn.functional as F
from tqdm import tqdm
from collections import defaultdict

from dgl import function as fn
from dgl.ops import edge_softmax
from dgl.utils import expand_as_pair


class HeteroGraphConv(nn.Module):
    def __init__(self, mods: dict):
        """

        :param mods: input modules for graph learning
        :param relation_aggregate: aggregate manner node features generated by different relations
        """
        super(HeteroGraphConv, self).__init__()
        self.mods = nn.ModuleDict(mods)

    def forward(self, graph: dgl.DGLHeteroGraph, input_src: dict, input_dst: dict, src_node_transformation: nn.ModuleDict,
                dst_node_transformation: nn.ModuleDict, src_node_attention: nn.ParameterDict, dst_node_attention: nn.ParameterDict):
        """
        call the forward function with each module.

        Parameters
        ----------
        graph : DGLHeteroGraph
            The Heterogeneous Graph.
        input_src : dict[str, Tensor], Input source node features {'ntype': features}.
        input_dst : dict[str, Tensor], Input destination node features {'ntype': features}.
        src_node_transformation: nn.ModuleDict, weights {'ntype', (input_dim, hidden_dim * heads)}
        dst_node_transformation: nn.ModuleDict, weights {'ntype', (input_dim, hidden_dim * heads)}
        src_node_attention: nn.ParameterDict, weights {'ntype', (1, num_heads, out_size)}
        dst_node_attention: nn.ParameterDict, weights {'ntype', (1, num_heads, out_size)}

        Returns
        -------
        outputs, dict[str, Tensor]
            Output representations for each type of destination node -> {dtype: features}.
        """

        # key: dsttype, value: list of representations
        outputs = defaultdict(list)

        for stype, etype, dtype in graph.canonical_etypes:
            rel_graph = graph[stype, etype, dtype]
            if rel_graph.number_of_edges() == 0:
                continue

            # shape (dst_nodes, hid_dim)
            outputs[dtype].append(self.mods[etype](rel_graph, (input_src[stype], input_dst[dtype]), src_node_transformation[stype], dst_node_transformation[dtype],
                                                   src_node_attention[stype], dst_node_attention[dtype]).flatten(start_dim=1))

        output_features = {}
        for ntype in outputs:
            if len(outputs[ntype]) == 1:
                output_features[ntype] = outputs[ntype][0]
            else:
                output_features[ntype] = torch.mean(torch.stack(outputs[ntype], dim=0), dim=0)

        return output_features


class GATConv(nn.Module):
    def __init__(self, in_feats: int, out_feats: int, num_heads: int, feat_drop: float = 0.0, use_attn_dst: bool = True, use_symmetric_norm: bool = False):
        """

        :param in_feats:
        :param out_feats:
        :param num_heads:
        :param feat_drop:
        :param use_attn_dst: whether calculate attention for destination node
        :param use_symmetric_norm: whether use use symmetric norm
        """
        super(GATConv, self).__init__()
        self._num_heads = num_heads
        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)
        self._out_feats = out_feats

        self.feat_dropout = nn.Dropout(feat_drop)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)

        self.use_attn_dst = use_attn_dst
        self._use_symmetric_norm = use_symmetric_norm

    def forward(self, graph: dgl.DGLHeteroGraph, feat: torch.Tensor or tuple, src_node_transformation: nn.Module, dst_node_transformation: nn.Module,
                src_node_attention: nn.Parameter, dst_node_attention: nn.Parameter):
        """
        graph : The graph
        feat : torch.Tensor or pair of torch.Tensor
        src_node_transformation : nn.Parameter, trainable parameters for source node feature transformation
        dst_node_transformation : nn.Parameter, trainable parameters for destination node feature transformation
        src_node_attention : nn.Parameter, trainable attention vector for source node
        dst_node_attention : nn.Parameter, trainable attention vector for destination node
        """

        with graph.local_scope():
            if isinstance(feat, tuple):
                h_src = self.feat_dropout(feat[0])
                h_dst = self.feat_dropout(feat[1])
                feat_src = src_node_transformation(h_src).view(-1, self._num_heads, self._out_feats)
                feat_dst = dst_node_transformation(h_dst).view(-1, self._num_heads, self._out_feats)
            else:
                h_src = h_dst = self.feat_dropout(feat)
                feat_src = feat_dst = src_node_transformation(h_src).view(-1, self._num_heads, self._out_feats)
                if graph.is_block:
                    feat_dst = feat_src[:graph.number_of_dst_nodes()]

            if self._use_symmetric_norm:
                degs = graph.out_degrees().float().clamp(min=1)
                norm = torch.pow(degs, -0.5)
                shp = norm.shape + (1,) * (feat_src.dim() - 1)
                norm = torch.reshape(norm, shp)
                feat_src = feat_src * norm

            el = (feat_src * src_node_attention).sum(dim=-1).unsqueeze(-1)
            graph.srcdata.update({'ft': feat_src, 'el': el})

            if self.use_attn_dst:
                er = (feat_dst * dst_node_attention).sum(dim=-1).unsqueeze(-1)
                graph.dstdata.update({'er': er})
                # compute edge attention, el and er are a_l Wh_i and a_r Wh_j
                graph.apply_edges(fn.u_add_v('el', 'er', 'e'))
            else:
                graph.apply_edges(fn.copy_u("el", "e"))

            e = self.leaky_relu(graph.edata.pop('e'))
            graph.edata["a"] = edge_softmax(graph, e)

            # message passing
            graph.update_all(fn.u_mul_e('ft', 'a', 'm'), fn.sum('m', 'ft'))
            rst = graph.dstdata['ft']

            if self._use_symmetric_norm:
                degs = graph.in_degrees().float().clamp(min=1)
                # norm = torch.pow(degs, -0.5)
                norm = torch.pow(degs, 0.5)
                shp = norm.shape + (1,) * (feat_dst.dim() - 1)
                norm = torch.reshape(norm, shp)
                rst = rst * norm

            return rst


class LEGATLayer(nn.Module):
    def __init__(self, in_size: int, out_size: int, etypes: list, ntypes: list, num_heads: int = 8, residual: bool = False, feat_drop: float = 0.0,
                 output_drop: float = 0.0, use_attn_dst: bool = True, norm: bool = True, full_batch: bool = True, use_symmetric_norm: bool = False):
        """
        :param in_size: input feature dimension
        :param out_size: output feature dimension
        :param etypes:  list of relation types
        :param ntypes:  list of node types
        :param num_heads: number of attention heads
        :param residual: Boolean, whether to consider self information
        :param feat_drop: Feature dropout probability
        :param output_drop: Output dropout probability
        :param use_attn_dst: whether calculate attention for destination node
        :param norm: Boolean, whether normalization
        :param full_batch: Whether to train in a full-batch manner
        :param use_symmetric_norm: whether use use symmetric norm
        """

        super(LEGATLayer, self).__init__()

        self.in_size = in_size
        self.out_size = out_size
        self.etypes = etypes
        self.ntypes = ntypes
        self.num_heads = num_heads
        self.residual = residual
        self.feat_drop = feat_drop
        self.norm = norm
        self.full_batch = full_batch
        self.output_dropout = nn.Dropout(output_drop)

        self.src_node_transformation = nn.ModuleDict({
            ntype: nn.Linear(in_size, out_size * num_heads, bias=False) for ntype in ntypes
        })

        self.dst_node_transformation = nn.ModuleDict({
            ntype: nn.Linear(in_size, out_size * num_heads, bias=False) for ntype in ntypes
        })

        self.src_node_attention = nn.ParameterDict({
            ntype: nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_size))) for ntype in ntypes
        })

        self.dst_node_attention = nn.ParameterDict({
            ntype: nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_size))) for ntype in ntypes
        })

        if self.residual:
            # residual connection
            self.res_fc = nn.ModuleDict({
                ntype: nn.Linear(in_size, out_size * num_heads, bias=False) for ntype in ntypes
            })

        if self.norm:
            self.normalization = nn.ModuleDict({
                ntype: nn.BatchNorm1d(out_size * num_heads) for ntype in ntypes
            })

        self.hetero_conv = HeteroGraphConv({
            etype: GATConv(in_feats=in_size, out_feats=out_size, num_heads=num_heads, feat_drop=feat_drop,
                           use_attn_dst=use_attn_dst, use_symmetric_norm=use_symmetric_norm) for etype in etypes
        })

        self.reset_parameters()

    def reset_parameters(self):
        """
        Reinitialize learnable parameters.
        """
        gain = nn.init.calculate_gain('relu')
        for ntype in self.src_node_transformation:
            nn.init.xavier_normal_(self.src_node_transformation[ntype].weight, gain=gain)
        for ntype in self.dst_node_transformation:
            nn.init.xavier_normal_(self.dst_node_transformation[ntype].weight, gain=gain)
        for weight in self.src_node_attention:
            nn.init.xavier_normal_(self.src_node_attention[weight], gain=gain)
        for weight in self.dst_node_attention:
            nn.init.xavier_normal_(self.dst_node_attention[weight], gain=gain)
        if self.residual:
            for ntype in self.res_fc:
                nn.init.xavier_uniform_(self.res_fc[ntype].weight, gain=gain)

    def forward(self, graph: dgl.DGLGraph, node_features: dict):
        """
        :param graph: a graph
        :param node_features: tensor, Input features， (N, in_size)
        :return: (N, num_heads * out_size)
        """
        # dictionary of input source features and destination features
        input_src = node_features

        if self.full_batch:
            input_dst = node_features
        else:
            input_dst = {}
            for ntype in node_features:
                input_dst[ntype] = node_features[ntype][:graph.number_of_dst_nodes(ntype)]

        output_features = self.hetero_conv(graph, input_src, input_dst, self.src_node_transformation, self.dst_node_transformation,
                                           self.src_node_attention, self.dst_node_attention)

        if self.residual:
            for ntype in output_features:
                output_features[ntype] = output_features[ntype] + self.res_fc[ntype](input_dst[ntype])

        if self.norm:
            for ntype in output_features:
                output_features[ntype] = self.normalization[ntype](output_features[ntype])

        for ntype in output_features:
            output_features[ntype] = self.output_dropout(F.relu(output_features[ntype]))

        # Tensor, shape (N, out_size * num_heads)
        return output_features


class LEGAT(nn.Module):
    def __init__(self, input_dim_dict: dict, hidden_sizes: list, etypes: list, ntypes: list, num_heads: int = 8, residual: bool = False, input_drop: float = 0.0,
                 feat_drop: float = 0.0, output_drop: float = 0.0, use_attn_dst: bool = True, norm: bool = True, use_symmetric_norm: bool = False, full_batch: bool = True):
        """
        :param input_dim_dict: dict, input dim dictionary
        :param hidden_sizes: list, hidden feature dimension of each layer
        :param etypes:  list of relation types
        :param ntypes:  list of node types
        :param num_heads: int, number of attention heads
        :param residual: Boolean, whether to consider self information
        :param input_drop: Input dropout probability
        :param feat_drop: Feature dropout probability
        :param output_drop: Dropout probability
        :param use_attn_dst: whether calculate attention for destination node
        :param norm: Boolean, whether normalization
        :param full_batch: Whether to train in a full-batch manner
        :param use_symmetric_norm: whether use use symmetric norm
        """

        super(LEGAT, self).__init__()

        self.input_dim_dict = input_dim_dict
        self.hidden_sizes = hidden_sizes
        self.etypes = etypes
        self.ntypes = ntypes
        self.num_heads = num_heads
        self.residual = residual
        self.feat_drop = feat_drop
        self.output_drop = output_drop
        self.norm = norm
        self.full_batch = full_batch

        self.input_dropout = nn.Dropout(input_drop)

        # align the dimension of different types of nodes
        self.projection_layer = nn.ModuleDict({
            ntype: nn.Linear(input_dim_dict[ntype], hidden_sizes[0] * num_heads, bias=False) for ntype in input_dim_dict
        })

        # each layer takes in the heterogeneous graph as input
        self.layers = nn.ModuleList()
        self.layers.append(LEGATLayer(hidden_sizes[0] * num_heads, hidden_sizes[0], etypes=etypes, ntypes=ntypes, num_heads=num_heads, residual=residual,
                                      feat_drop=feat_drop, output_drop=output_drop, use_attn_dst=use_attn_dst, norm=norm, full_batch=full_batch, use_symmetric_norm=use_symmetric_norm))

        for l in range(1, len(hidden_sizes)):
            self.layers.append(LEGATLayer(hidden_sizes[l-1] * num_heads, hidden_sizes[l], etypes=etypes, ntypes=ntypes, num_heads=num_heads, residual=residual,
                                          feat_drop=feat_drop, output_drop=output_drop, use_attn_dst=use_attn_dst, norm=norm, full_batch=full_batch, use_symmetric_norm=use_symmetric_norm))

    def forward(self, blocks: list or dgl.DGLHeteroGraph, node_features: dict):
        """
        :param blocks: list of sampled dgl.DGLHeteroGraph
        :param node_features: node features, dict, {"type": features}
        :return:
        """
        # feature projection
        for ntype in node_features:
            node_features[ntype] = self.projection_layer[ntype](self.input_dropout(node_features[ntype]))

        if self.full_batch:
            for layer in self.layers:
                node_features = layer(blocks, node_features)
        else:
            for block, layer in zip(blocks, self.layers):
                node_features = layer(block, node_features)

        return node_features

    def inference(self, graph: dgl.DGLHeteroGraph, node_features: dict, device: str):
        """
        mini-batch inference of final representation over all node types. Outer loop: Interate the layers, Inner loop: Interate the batches

        :param graph: The whole relational graphs
        :param node_features: features of all the nodes in the whole graph, dict, {"type": features}
        :param device: device str
        """
        with torch.no_grad():
            # interate over each layer
            for index, layer in enumerate(self.layers):
                # Tensor, features of all types of nodes, store on cpu
                y = {
                    ntype: torch.zeros(
                        graph.number_of_nodes(ntype), self.num_heads * self.hidden_sizes[index]) for ntype in graph.ntypes
                }
                # full sample for each type of nodes
                sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)
                dataloader = dgl.dataloading.NodeDataLoader(
                    graph,
                    {ntype: torch.arange(graph.number_of_nodes(ntype)) for ntype in graph.ntypes},
                    sampler,
                    batch_size=1280,
                    shuffle=True,
                    drop_last=False,
                    num_workers=4)

                tqdm_dataloader = tqdm(dataloader, ncols=120)
                for batch, (input_nodes, output_nodes, blocks) in enumerate(tqdm_dataloader):
                    block = blocks[0].to(device)

                    input_features = {ntype: node_features[ntype][input_nodes[ntype]].to(device) for ntype in input_nodes.keys()}

                    if index == 0:
                        # input drop and feature projection for the first layer in the full batch inference
                        for ntype in input_features:
                            input_features[ntype] = self.projection_layer[ntype](self.input_dropout(input_features[ntype]))

                    h = layer(block, input_features)

                    for k in h.keys():
                        y[k][output_nodes[k]] = h[k].cpu()

                    tqdm_dataloader.set_description(f'inference for the {batch}-th batch in model {index}-th layer')

                # update the features of all the nodes (after the graph convolution) in the whole graph
                node_features = y

        return y
